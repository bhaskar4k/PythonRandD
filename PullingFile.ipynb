{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59872cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sample\n",
      "shape: (5, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Loan ID â”† Loan No.    â”† Org ID â”† Org Name â”† Sanction Date       â”† Current max DPD     â”† Lifetime max DPD    â”† Lifetime max DPD on â”† Total Overdue       â”† Principal Overdue   â”† EMI Overdue         â”† POS                 â”† Current max DPD_1   â”† Lifetime max DPD_1  â”† Lifetime max DPD on_1 â”† Total Overdue_1     â”† Principal Overdue_1 â”† EMI Overdue_1       â”† POS_1               â”‚\n",
      "â”‚ ---     â”† ---         â”† ---    â”† ---      â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                   â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”‚\n",
      "â”‚ str     â”† str         â”† str    â”† null     â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                   â”† str                 â”† str                 â”† str                 â”† str                 â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ null    â”† null        â”† null   â”† null     â”† null                â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00   â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”‚\n",
      "â”‚ 535345  â”† OXYTL01ZUBT â”† ABC123 â”† null     â”† 2021-12-27 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 1                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 1                   â”† 1                   â”† 1                   â”† 0                   â”‚\n",
      "â”‚ 657557  â”† OXYTL01JEP3 â”† ABC124 â”† null     â”† 2021-12-10 00:00:00 â”† 0                   â”† 36                  â”† 2022-10-09 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 119134              â”† 2                   â”† 27                  â”† 2022-10-09 00:00:00   â”† 2                   â”† 3                   â”† 2                   â”† 119134              â”‚\n",
      "â”‚ 323232  â”† OXYTL0116YS â”† ABC125 â”† null     â”† 2021-12-11 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 2                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 2                   â”† 2                   â”† 3                   â”† 0                   â”‚\n",
      "â”‚ 856345  â”† OXYTL01TJ75 â”† ABC126 â”† null     â”† 2021-12-11 00:00:00 â”† 0                   â”† 26                  â”† 2024-01-29 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 168741              â”† 2                   â”† 27                  â”† 2024-01-29 00:00:00   â”† 3                   â”† 1                   â”† 4                   â”† 168741              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Target rows per partition â‰ˆ 351,263 \n",
      "Reading rows 0 â†’ 351,262  1764666693.822532\n",
      "df_chunk\n",
      "shape: (5, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Loan ID â”† Loan No.    â”† Org ID â”† Org Name â”† Sanction Date       â”† Current max DPD     â”† Lifetime max DPD    â”† Lifetime max DPD on â”† Total Overdue       â”† Principal Overdue   â”† EMI Overdue         â”† POS                 â”† Current max DPD_1   â”† Lifetime max DPD_1  â”† Lifetime max DPD on_1 â”† Total Overdue_1     â”† Principal Overdue_1 â”† EMI Overdue_1       â”† POS_1               â”‚\n",
      "â”‚ ---     â”† ---         â”† ---    â”† ---      â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                   â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”‚\n",
      "â”‚ str     â”† str         â”† str    â”† null     â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                   â”† str                 â”† str                 â”† str                 â”† str                 â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ null    â”† null        â”† null   â”† null     â”† null                â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00   â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”‚\n",
      "â”‚ 535345  â”† OXYTL01ZUBT â”† ABC123 â”† null     â”† 2021-12-27 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 1                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 1                   â”† 1                   â”† 1                   â”† 0                   â”‚\n",
      "â”‚ 657557  â”† OXYTL01JEP3 â”† ABC124 â”† null     â”† 2021-12-10 00:00:00 â”† 0                   â”† 36                  â”† 2022-10-09 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 119134              â”† 2                   â”† 27                  â”† 2022-10-09 00:00:00   â”† 2                   â”† 3                   â”† 2                   â”† 119134              â”‚\n",
      "â”‚ 323232  â”† OXYTL0116YS â”† ABC125 â”† null     â”† 2021-12-11 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 2                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 2                   â”† 2                   â”† 3                   â”† 0                   â”‚\n",
      "â”‚ 856345  â”† OXYTL01TJ75 â”† ABC126 â”† null     â”† 2021-12-11 00:00:00 â”† 0                   â”† 26                  â”† 2024-01-29 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 168741              â”† 2                   â”† 27                  â”† 2024-01-29 00:00:00   â”† 3                   â”† 1                   â”† 4                   â”† 168741              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "External error: Schema error: Invalid data type for Delta Lake: Null",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 350\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# MakeParquetChunks(\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m#     \"abc\",\u001b[39;00m\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m#     \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\Excels\\\\OxyzoDummyTemplateTransformed.csv\",\u001b[39;00m\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m#     \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m#     False\u001b[39;00m\n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    331\u001b[39m pl.Config.set_tbl_rows(\u001b[32m50000\u001b[39m)         \u001b[38;5;66;03m# show up to 50k rows\u001b[39;00m\n\u001b[32m    332\u001b[39m pl.Config.set_tbl_cols(\u001b[32m500\u001b[39m) \n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43mMakeParquetChunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mDev\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mOxyzo R&D\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mRndGit\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mOxyzoDummyTemplateExcel.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mDev\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mOxyzo R&D\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mRndGit\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mSourceDeltaLake\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 310\u001b[39m, in \u001b[36mMakeParquetChunks\u001b[39m\u001b[34m(newly_created_data_file_guid, origin_filepath, delta_dir_network_server_path, save_temporary_deltalake_and_copy)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_chunk.is_empty():\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mwrite_deltalake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfirst_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmerge\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    315\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m first_chunk = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ†’ Written partition #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_chunk)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\deltalake\\writer\\writer.py:147\u001b[39m, in \u001b[36mwrite_deltalake\u001b[39m\u001b[34m(table_or_uri, data, partition_by, mode, name, description, configuration, schema_mode, storage_options, predicate, target_file_size, writer_properties, post_commithook_properties, commit_properties)\u001b[39m\n\u001b[32m    131\u001b[39m     table._table.write(\n\u001b[32m    132\u001b[39m         data=data,\n\u001b[32m    133\u001b[39m         batch_schema=compatible_delta_schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m         post_commithook_properties=post_commithook_properties,\n\u001b[32m    145\u001b[39m     )\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[43mwrite_deltalake_rust\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompatible_delta_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_file_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_file_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommit_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_commithook_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_commithook_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mException\u001b[39m: External error: Schema error: Invalid data type for Delta Lake: Null"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from deltalake import write_deltalake\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook, utils\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def DetectFileType(filepath: str) -> str:\n",
    "    ext = Path(filepath).suffix.lower()\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext in [\".xlsx\", \".xls\", \".xlsm\", \".xlsb\"]:\n",
    "        return \"excel\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from typing import Dict, List\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def ReadExcelFile(\n",
    "    origin_filepath: str,\n",
    "    skip_rows: int,\n",
    "    n_rows: int,\n",
    "    schema_dict: Dict[str, pl.DataType] | None = None,\n",
    "    base_columns: List[str] | None = None,\n",
    ") -> pl.DataFrame:\n",
    "    try:\n",
    "        wb = load_workbook(origin_filepath, read_only=False, data_only=True)\n",
    "        sh = wb[wb.sheetnames[0]]\n",
    "\n",
    "        # ===== MERGED-CELL MAP =====\n",
    "        merged_map = {}\n",
    "        for rng in sh.merged_cells. ranges:\n",
    "            master = sh.cell(rng.min_row, rng.min_col).value\n",
    "            for r in range(rng.min_row, rng.max_row + 1):\n",
    "                for c in range(rng.min_col, rng.max_col + 1):\n",
    "                    merged_map[(r, c)] = master\n",
    "\n",
    "        # ===== HEADERS =====\n",
    "        header_row = skip_rows + 1\n",
    "\n",
    "        raw_headers = []\n",
    "        for c in range(1, sh.max_column + 1):\n",
    "            cell = sh.cell(header_row, c)\n",
    "            val = merged_map.get((cell.row, cell.column), cell.value)\n",
    "            raw_headers.append(str(val) if val else f\"column_{c}\")\n",
    "\n",
    "        # ===== DUPLICATE HANDLING WITH NUMERIC SUFFIX =====\n",
    "        col_names, cnt = [], {}\n",
    "        for h in raw_headers:\n",
    "            if h not in cnt:\n",
    "                cnt[h] = 0\n",
    "                col_names.append(h)\n",
    "            else:\n",
    "                cnt[h] += 1\n",
    "                col_names. append(f\"{h}_{cnt[h]}\")\n",
    "\n",
    "        # ===== FIND REAL LAST NON-EMPTY ROW =====\n",
    "        real_last_row = 0\n",
    "        for r in range(header_row + 1, sh.max_row + 1):\n",
    "            row_values = [\n",
    "                merged_map.get((r, c), sh.cell(r, c).value)\n",
    "                for c in range(1, sh.max_column + 1)\n",
    "            ]\n",
    "            if any(v not in (None, \"\") for v in row_values):\n",
    "                real_last_row = r\n",
    "            else:\n",
    "                break  # first empty row â†’ stop scanning\n",
    "\n",
    "        if real_last_row == 0:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "        # ===== SAFE END ROW =====\n",
    "        data_start = header_row + 1\n",
    "        data_end = min(data_start + n_rows - 1, real_last_row)\n",
    "\n",
    "        # ===== READ DATA WITH MERGED CELL VALUES FILLED =====\n",
    "        columns = {i: [] for i in range(len(col_names))}\n",
    "\n",
    "        for row in sh.iter_rows(min_row=data_start, max_row=data_end, values_only=False):\n",
    "            for col_idx, cell in enumerate(row):\n",
    "                if col_idx >= len(col_names):\n",
    "                    break\n",
    "                # Get value from merged_map if cell is merged, otherwise use cell value\n",
    "                v = merged_map.get((cell.row, cell.column), cell.value)\n",
    "                columns[col_idx].append(v)\n",
    "\n",
    "        wb.close()\n",
    "\n",
    "        # ===== NORMALIZE COLUMN LENGTHS =====\n",
    "        max_len = max((len(col) for col in columns.values()), default=0)\n",
    "        for col in columns. values():\n",
    "            col.extend([None] * (max_len - len(col)))\n",
    "\n",
    "        # ===== STRINGIFY AND CREATE DATAFRAME =====\n",
    "        final_data = {\n",
    "            col_names[i]: [str(v) if v is not None else None for v in columns[i]]\n",
    "            for i in columns\n",
    "        }\n",
    "\n",
    "        df = pl.DataFrame(final_data, infer_schema_length=0)\n",
    "\n",
    "        # ===== APPLY SCHEMA IF PROVIDED =====\n",
    "        if schema_dict:\n",
    "            for col, dtype in schema_dict. items():\n",
    "                if col in df.columns:\n",
    "                    df = df.with_columns(pl.col(col). cast(dtype, strict=False))\n",
    "\n",
    "        # ===== SELECT BASE COLUMNS IF PROVIDED =====\n",
    "        if base_columns:\n",
    "            df = df. select([c for c in base_columns if c in df.columns])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error:\", e)\n",
    "        return pl.DataFrame()\n",
    "\n",
    "\n",
    "# def ExcelReadChunkAndApplySchema(\n",
    "#     origin_filepath: str,\n",
    "#     skip_rows: int,\n",
    "#     n_rows: int,\n",
    "#     schema_dict: Dict[str, pl.DataType] | None = None,\n",
    "#     base_columns: List[str] | None = None,\n",
    "# ) -> pl.DataFrame:\n",
    "#     try:\n",
    "#         # ===== 1) Build merged cell map =====\n",
    "#         workbook = load_workbook(filename=origin_filepath, read_only=False, data_only=True)\n",
    "#         sheet = workbook[workbook.sheetnames[0]]\n",
    "\n",
    "#         merged_map = {}\n",
    "#         for merged_range in sheet.merged_cells.ranges:\n",
    "#             master_value = sheet.cell(row=merged_range.min_row, column=merged_range.min_col).value\n",
    "#             for r in range(merged_range.min_row, merged_range.max_row + 1):\n",
    "#                 for c in range(merged_range.min_col, merged_range.max_col + 1):\n",
    "#                     merged_map[(r, c)] = master_value\n",
    "\n",
    "#         # Read header row\n",
    "#         header_row_num = skip_rows + 1\n",
    "#         header_row = sheet[header_row_num]\n",
    "#         column_names = []\n",
    "#         for col_idx, cell in enumerate(header_row):\n",
    "#             value = merged_map.get((cell.row, cell.column), cell.value)\n",
    "#             column_names.append(str(value) if value else f\"column_{col_idx + 1}\")\n",
    "\n",
    "#         workbook.close()\n",
    "\n",
    "#         # ===== 2) Read chunk with merged values =====\n",
    "#         workbook = load_workbook(filename=origin_filepath, read_only=True, data_only=True)\n",
    "#         sheet = workbook[workbook.sheetnames[0]]\n",
    "\n",
    "#         start_row = skip_rows + 1\n",
    "#         end_row = start_row + n_rows\n",
    "\n",
    "#         col_data = {}\n",
    "        \n",
    "#         for row_idx, row in enumerate(sheet.iter_rows(min_row=start_row, max_row=end_row, values_only=False)):\n",
    "#             if row_idx >= n_rows:\n",
    "#                 break\n",
    "            \n",
    "#             for col_idx, cell in enumerate(row):\n",
    "#                 if col_idx not in col_data:\n",
    "#                     col_data[col_idx] = []\n",
    "                \n",
    "#                 value = merged_map.get((cell.row, cell.column), cell.value)\n",
    "#                 col_data[col_idx].append(value)\n",
    "\n",
    "#         workbook.close()\n",
    "\n",
    "#         if not col_data:\n",
    "#             return pl.DataFrame()\n",
    "\n",
    "#         # ===== 3) Normalize column lengths =====\n",
    "#         # Because columns might have different lengths.\n",
    "\n",
    "#         # Example:\n",
    "#         # Column 0: 100 rows\n",
    "#         # Column 1: 98 rows\n",
    "#         # Column 2: 100 rows\n",
    "\n",
    "#         # Polars needs all columns to have the same length. This code:\n",
    "#         # max_rows = max(...) â†’ finds the longest column (100)\n",
    "#         # col_list.extend([None] * ...) â†’ pads shorter columns with None to reach 100\n",
    "#         # max_rows = max(len(col) for col in col_data.values())\n",
    "#         # for col_list in col_data.values():\n",
    "#         #     col_list.extend([None] * (max_rows - len(col_list)))\n",
    "\n",
    "#         # ===== 4) Create DataFrame =====\n",
    "#         data_dict = {column_names[col_idx]: col_data[col_idx] for col_idx in sorted(col_data.keys())}\n",
    "#         df = pl.DataFrame(data_dict, strict=False)\n",
    "\n",
    "#         # ===== 5) Apply schema if provided =====\n",
    "#         if schema_dict:\n",
    "#             for col, dtype in schema_dict.items():\n",
    "#                 if col in df.columns:\n",
    "#                     df = df.with_columns(pl.col(col).cast(dtype, strict=False))\n",
    "\n",
    "#         # ===== 6) Select base columns if provided =====\n",
    "#         if base_columns:\n",
    "#             df = df.select([c for c in base_columns if c in df.columns])\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error reading Excel chunk: {e}\")\n",
    "#         return pl.DataFrame()\n",
    "    \n",
    "\n",
    "def MakeParquetChunks(\n",
    "    newly_created_data_file_guid: str,\n",
    "    origin_filepath: str,\n",
    "    delta_dir_network_server_path: str,\n",
    "    save_temporary_deltalake_and_copy: bool\n",
    ") -> str:\n",
    "    target_mb_per_partition: int = 512\n",
    "    skip_rows: int = 0\n",
    "    max_sample_rows: int = 50_000\n",
    "    buffer_rows: int = 50_000\n",
    "\n",
    "    # Validate inputs\n",
    "    if not origin_filepath or not os.path.isfile(origin_filepath):\n",
    "        raise FileNotFoundError(f\"Origin CSV not found: {origin_filepath}\")\n",
    "    \n",
    "    # Temporary local delta lake directory because delta lake is not being saved in network address\n",
    "    if save_temporary_deltalake_and_copy == True:\n",
    "        delta_dir = \"C:\\\\TemporaryDeltaLake\\\\\" + newly_created_data_file_guid\n",
    "    else:\n",
    "        delta_dir = delta_dir_network_server_path\n",
    "\n",
    "    # Directories preparation\n",
    "    os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Lazy scan CSV for random sample\n",
    "    # -----------------------------\n",
    "    #lf = pl.scan_csv(origin_filepath, ignore_errors=True)\n",
    "    ## Read some rows while skipping some rows to get a better sample\n",
    "    Extension = DetectFileType(origin_filepath)\n",
    "\n",
    "    if Extension == \"csv\":\n",
    "        df_sample = pl.read_csv(origin_filepath, skip_rows=skip_rows, n_rows=max_sample_rows, ignore_errors=True)\n",
    "    elif Extension == \"excel\":\n",
    "        df_sample = ReadExcelFile(origin_filepath, skip_rows, max_sample_rows, None, None)\n",
    "\n",
    "    print(\"df_sample\")\n",
    "    print(df_sample)\n",
    "\n",
    "    ## Getting the base schema\n",
    "    schema_dict = {col: dtype for col, dtype in zip(df_sample.columns, df_sample.dtypes)}\n",
    "    base_columns = list(schema_dict.keys())\n",
    " \n",
    "    # Estimate partition size\n",
    "    temp_path = \"temp_sample.parquet\"\n",
    "    df_sample.write_parquet(temp_path)\n",
    "    row_size = os.path.getsize(temp_path) / len(df_sample)\n",
    "    os.remove(temp_path)\n",
    "    rows_per_partition = max(int((target_mb_per_partition * 1024 * 1024) / row_size), buffer_rows)\n",
    "    print(f\"Target rows per partition â‰ˆ {rows_per_partition:,} \")\n",
    " \n",
    "    # -----------------------------\n",
    "    # Streaming CSV read + Delta write\n",
    "    # -----------------------------\n",
    "    first_chunk = True\n",
    "    pos = 0\n",
    "    chunk_idx = 1\n",
    " \n",
    "    ## Since the file is huge we read in chunks and write directly to Delta\n",
    "    while True:\n",
    "        print(f\"Reading rows {pos:,} â†’ {pos + rows_per_partition - 1:,} \",time.time())\n",
    "        if Extension == \"csv\":\n",
    "            df_chunk = pl.read_csv(\n",
    "                origin_filepath,\n",
    "                skip_rows=pos,\n",
    "                n_rows=rows_per_partition,\n",
    "                dtypes=schema_dict,\n",
    "                ignore_errors=True\n",
    "            ).select(base_columns)\n",
    "        elif Extension == \"excel\":\n",
    "            df_chunk = ReadExcelFile(\n",
    "                origin_filepath,\n",
    "                skip_rows=pos,\n",
    "                n_rows=rows_per_partition,\n",
    "                schema_dict=schema_dict,\n",
    "                base_columns=base_columns\n",
    "            )\n",
    "\n",
    "        print(\"df_chunk\")\n",
    "        print(df_chunk)\n",
    " \n",
    "        if df_chunk.is_empty():\n",
    "            break\n",
    " \n",
    "        write_deltalake(\n",
    "            delta_dir,\n",
    "            df_chunk,\n",
    "            mode=\"overwrite\" if first_chunk else \"append\",\n",
    "            schema_mode=\"merge\"\n",
    "        )\n",
    "        first_chunk = False\n",
    "        print(f\"â†’ Written partition #{chunk_idx} with {len(df_chunk):,} rows\")\n",
    "        pos += len(df_chunk)\n",
    "        chunk_idx += 1\n",
    " \n",
    "    print(\"\\n==========================================\")\n",
    "    print(\" FULLY STREAMING CSV â†’ DELTA COMPLETED ðŸŽ‰\")\n",
    "    print(\" Delta path:\", delta_dir)\n",
    "    print(\"==========================================\\n\")\n",
    "    return delta_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    pl.Config.set_fmt_str_lengths(2000)   # widen column display\n",
    "    pl.Config.set_tbl_width_chars(2000)   # expand table width\n",
    "    pl.Config.set_tbl_rows(50000)         # show up to 50k rows\n",
    "    pl.Config.set_tbl_cols(500) \n",
    "\n",
    "    MakeParquetChunks(\n",
    "        \"abc\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\OxyzoDummyTemplateExcel.xlsx\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\n",
    "        False\n",
    "    )\n",
    "\n",
    "    # MakeParquetChunks(\n",
    "    #     \"abc\",\n",
    "    #     \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\Excels\\\\OxyzoDummyTemplateTransformed.csv\",\n",
    "    #     \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\n",
    "    #     False\n",
    "    # )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumeration for data types.\"\"\"\n",
    "    Unknown = 0\n",
    "    Numeric = 1\n",
    "    DateTime = 2\n",
    "    Float = 3\n",
    "    String = 4\n",
    "    Formula = 5\n",
    "    Blank = 6\n",
    "    Boolean = 7\n",
    "    Error = 8\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def map_pyarrow_type(dtype: pa.DataType) -> int:\n",
    "    if pa.types.is_integer(dtype) or pa.types.is_decimal(dtype):\n",
    "        return DataType.Numeric.value\n",
    "    if pa.types.is_floating(dtype):\n",
    "        return DataType.Float.value\n",
    "    if pa.types.is_string(dtype):\n",
    "        return DataType.String.value\n",
    "    if pa.types.is_boolean(dtype):\n",
    "        return DataType.Boolean.value\n",
    "    if pa.types.is_timestamp(dtype) or pa.types.is_date(dtype):\n",
    "        return DataType.DateTime.value\n",
    "    return DataType.Unknown.value\n",
    "\n",
    "\n",
    "def GetFullDetailsOfDeltaLakeFile(folder_path: str):\n",
    "    \"\"\"\n",
    "    Get comprehensive metadata about a Delta Lake table\n",
    "    \n",
    "    Returns:\n",
    "    {\n",
    "        \"StgDataFileInfo\": {\n",
    "            \"RowCount\": int,\n",
    "            \"ColumnCount\": int\n",
    "        },\n",
    "        \"StgDataFileColumnInfo\": [\n",
    "            {\n",
    "                \"DataFileColumnName\": str,\n",
    "                \"ColumnType\": str,\n",
    "                \"HasNullValue\": bool,\n",
    "                \"DistinctValueCount\": int,\n",
    "                \"NullCount\": int\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dt = DeltaTable(folder_path)\n",
    "        \n",
    "        # Get schema and dataset\n",
    "        dataset = dt.to_pyarrow_dataset()\n",
    "        arrow_schema = dataset.schema\n",
    "\n",
    "        # Initialize column info\n",
    "        col_info = {}\n",
    "        for f in arrow_schema:\n",
    "            col_info[f.name] = {\n",
    "                \"DataFileColumnName\": f.name,\n",
    "                \"ColumnType\": map_pyarrow_type(f.type),\n",
    "                \"HasNullValue\": False,\n",
    "                \"NullCount\": 0,\n",
    "                \"DistinctValueCount\": set(),\n",
    "            }\n",
    "\n",
    "        # Scan batches\n",
    "        row_count = 0\n",
    "        scanner = ds.Scanner.from_dataset(dataset, columns=None, batch_size=50_000)\n",
    "\n",
    "        for batch in scanner.to_batches():\n",
    "            row_count += len(batch)\n",
    "            \n",
    "            for col_name, array in zip(batch.schema.names, batch.columns):\n",
    "                null_count = array.null_count\n",
    "                \n",
    "                if null_count > 0:\n",
    "                    col_info[col_name][\"HasNullValue\"] = True\n",
    "                    col_info[col_name][\"NullCount\"] += null_count\n",
    "\n",
    "                # Get distinct values (excluding nulls)\n",
    "                try:\n",
    "                    if len(array) > null_count:\n",
    "                        non_null_array = array.drop_null()\n",
    "                        uniques = pc.unique(non_null_array)\n",
    "                        col_info[col_name][\"DistinctValueCount\"].update(uniques.to_pylist())\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Convert distinct value sets to counts\n",
    "        for col in col_info.values():\n",
    "            col[\"DistinctValueCount\"] = len(col[\"DistinctValueCount\"])\n",
    "\n",
    "        # Get Delta table metadata\n",
    "        dt_metadata = dt.metadata()\n",
    "\n",
    "        return {\n",
    "            \"StgDataFileInfo\": {\n",
    "                \"RowCount\": row_count,\n",
    "                \"ColumnCount\": len(arrow_schema),\n",
    "            },\n",
    "            \"StgDataFileColumnInfo\": list(col_info.values()),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Delta Lake table: {e}\")\n",
    "        return {\n",
    "            \"StgDataFileInfo\": {\"RowCount\": 0, \"ColumnCount\": 0},\n",
    "            \"StgDataFileColumnInfo\": [],\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = GetFullDetailsOfDeltaLakeFile(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\")\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
