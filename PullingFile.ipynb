{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59872cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from deltalake import write_deltalake\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook, utils\n",
    "from typing import Dict, List\n",
    "\n",
    "def DetectFileType(filepath: str) -> str:\n",
    "    ext = Path(filepath).suffix.lower()\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext in [\".xlsx\", \".xls\", \".xlsm\", \".xlsb\"]:\n",
    "        return \"excel\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def ReadExcelFile(\n",
    "    origin_filepath: str,\n",
    "    skip_rows: int = 0,\n",
    "    max_sample_rows: int = 50_000\n",
    ") -> pl.DataFrame:\n",
    "    try:\n",
    "        # ===== 1) Load merged-cell metadata =====\n",
    "        meta_wb = load_workbook(\n",
    "            filename=origin_filepath,\n",
    "            read_only=False,  # Needed to read merged cell ranges\n",
    "            data_only=True    # Read actual values, not formulas\n",
    "        )\n",
    "        meta_ws = meta_wb[meta_wb.sheetnames[0]]\n",
    "\n",
    "        merged_map = {}\n",
    "        for merged_range in meta_ws.merged_cells.ranges:\n",
    "            min_row, max_row = merged_range.min_row, merged_range.max_row\n",
    "            min_col, max_col = merged_range.min_col, merged_range.max_col\n",
    "            master_value = meta_ws.cell(row=min_row, column=min_col).value\n",
    "\n",
    "            # Propagate master_value to all cells in merged range\n",
    "            for r in range(min_row, max_row + 1):\n",
    "                for c in range(min_col, max_col + 1):\n",
    "                    merged_map[(r, c)] = master_value\n",
    "\n",
    "        meta_wb.close()\n",
    "\n",
    "        # ===== 2) Stream sheet for efficiency =====\n",
    "        wb = load_workbook(\n",
    "            filename=origin_filepath,\n",
    "            read_only=True,\n",
    "            data_only=True\n",
    "        )\n",
    "        ws = wb[wb.sheetnames[0]]\n",
    "\n",
    "        start_row = skip_rows + 1\n",
    "        end_row = start_row + max_sample_rows - 1\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for row in ws.iter_rows(min_row=start_row, max_row=end_row):\n",
    "            row_values = []\n",
    "            for cell in row:\n",
    "                key = (cell.row, cell.column)\n",
    "                value = merged_map.get(key, cell.value)\n",
    "\n",
    "                # Normalize value: convert everything to string or None\n",
    "                if value is None:\n",
    "                    row_values.append(None)\n",
    "                else:\n",
    "                    row_values.append(str(value))\n",
    "\n",
    "            data.append(row_values)\n",
    "\n",
    "        wb.close()\n",
    "\n",
    "        # Normalize all rows to the same number of columns\n",
    "        if data:\n",
    "            max_cols = max(len(r) for r in data)\n",
    "            for r in data:\n",
    "                r.extend([None] * (max_cols - len(r)))\n",
    "\n",
    "            df_sample = pl.DataFrame(\n",
    "                data,\n",
    "                schema=[f\"column_{i+1}\" for i in range(max_cols)]\n",
    "            )\n",
    "        else:\n",
    "            df_sample = pl.DataFrame()\n",
    "\n",
    "        return df_sample\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the Polars-only Excel read: {e}\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "\n",
    "def ExcelReadChunk(\n",
    "    origin_filepath: str,\n",
    "    skip_rows: int,\n",
    "    n_rows: int,\n",
    "    schema_dict: Dict[str, pl.DataType] | None = None,\n",
    "    base_columns: List[str] | None = None,\n",
    ") -> pl.DataFrame:\n",
    "    try:\n",
    "        workbook = load_workbook(filename=origin_filepath, read_only=True, data_only=False)\n",
    "        sheet = workbook[workbook.sheetnames[0]]\n",
    "\n",
    "        # Build lookup table for merged-cell ranges\n",
    "        merged_map = {}\n",
    "        for merged_range in sheet.merged_cells.ranges:\n",
    "            min_row = merged_range.min_row\n",
    "            min_col = merged_range.min_col\n",
    "            # master cell (top-left of merged block)\n",
    "            master_value = sheet.cell(row=min_row, column=min_col).value\n",
    "\n",
    "            # Fill mapping for all cells in merged block\n",
    "            for r in range(merged_range.min_row, merged_range.max_row + 1):\n",
    "                for c in range(merged_range.min_col, merged_range.max_col + 1):\n",
    "                    merged_map[(r, c)] = master_value\n",
    "\n",
    "        start_row = skip_rows + 1\n",
    "        end_row = start_row + n_rows - 1\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for row in sheet.iter_rows(min_row=start_row, max_row=end_row):\n",
    "            row_values = []\n",
    "\n",
    "            for cell in row:\n",
    "                key = (cell.row, cell.column)\n",
    "\n",
    "                # If cell is merged â†’ use master value\n",
    "                if key in merged_map:\n",
    "                    row_values.append(merged_map[key])\n",
    "                else:\n",
    "                    row_values.append(cell.value)\n",
    "\n",
    "            data.append(row_values)\n",
    "\n",
    "        if not data:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "        df = pl.DataFrame(\n",
    "            data,\n",
    "            schema=[f\"column_{i+1}\" for i in range(len(data[0]))]\n",
    "        )\n",
    "\n",
    "        # Apply dtypes\n",
    "        if schema_dict:\n",
    "            for col, dtype in schema_dict.items():\n",
    "                if col in df.columns:\n",
    "                    df = df.with_columns(pl.col(col).cast(dtype, strict=False))\n",
    "\n",
    "        # Select base columns\n",
    "        if base_columns:\n",
    "            df = df.select([c for c in base_columns if c in df.columns])\n",
    "\n",
    "        if \"workbook\" in locals():\n",
    "            workbook.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading Excel chunk: {e}\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "\n",
    "def MakeParquetChunks(\n",
    "    newly_created_data_file_guid: str,\n",
    "    origin_filepath: str,\n",
    "    delta_dir_network_server_path: str,\n",
    "    save_temporary_deltalake_and_copy: bool\n",
    ") -> str:\n",
    "    target_mb_per_partition: int = 512\n",
    "    skip_rows: int = 0\n",
    "    max_sample_rows: int = 50_000\n",
    "    buffer_rows: int = 50_000\n",
    "\n",
    "    # Validate inputs\n",
    "    if not origin_filepath or not os.path.isfile(origin_filepath):\n",
    "        raise FileNotFoundError(f\"Origin CSV not found: {origin_filepath}\")\n",
    "    \n",
    "    # Temporary local delta lake directory because delta lake is not being saved in network address\n",
    "    if save_temporary_deltalake_and_copy == True:\n",
    "        delta_dir = \"C:\\\\TemporaryDeltaLake\\\\\" + newly_created_data_file_guid\n",
    "    else:\n",
    "        delta_dir = delta_dir_network_server_path\n",
    "\n",
    "    # Directories preparation\n",
    "    os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Lazy scan CSV for random sample\n",
    "    # -----------------------------\n",
    "    #lf = pl.scan_csv(origin_filepath, ignore_errors=True)\n",
    "    ## Read some rows while skipping some rows to get a better sample\n",
    "    Extension = DetectFileType(origin_filepath)\n",
    "\n",
    "    if Extension == \"csv\":\n",
    "        df_sample = pl.read_csv(origin_filepath, skip_rows=skip_rows, n_rows=max_sample_rows, ignore_errors=True)\n",
    "    elif Extension == \"excel\":\n",
    "        df_sample = ReadExcelFile(origin_filepath, skip_rows, max_sample_rows)\n",
    "\n",
    "    print(\"df_sample\")\n",
    "    print(df_sample)\n",
    "\n",
    "    ## Getting the base schema\n",
    "    schema_dict = {col: dtype for col, dtype in zip(df_sample.columns, df_sample.dtypes)}\n",
    "    base_columns = list(schema_dict.keys())\n",
    " \n",
    "    # Estimate partition size\n",
    "    temp_path = \"temp_sample.parquet\"\n",
    "    df_sample.write_parquet(temp_path)\n",
    "    row_size = os.path.getsize(temp_path) / len(df_sample)\n",
    "    os.remove(temp_path)\n",
    "    rows_per_partition = max(int((target_mb_per_partition * 1024 * 1024) / row_size), buffer_rows)\n",
    "    print(f\"Target rows per partition â‰ˆ {rows_per_partition:,} \")\n",
    " \n",
    "    # -----------------------------\n",
    "    # Streaming CSV read + Delta write\n",
    "    # -----------------------------\n",
    "    first_chunk = True\n",
    "    pos = 0\n",
    "    chunk_idx = 1\n",
    " \n",
    "    ## Since the file is huge we read in chunks and write directly to Delta\n",
    "    while True:\n",
    "        print(f\"Reading rows {pos:,} â†’ {pos + rows_per_partition - 1:,} \",time.time())\n",
    "        if Extension == \"csv\":\n",
    "            df_chunk = pl.read_csv(\n",
    "                origin_filepath,\n",
    "                skip_rows=pos,\n",
    "                n_rows=rows_per_partition,\n",
    "                dtypes=schema_dict,\n",
    "                ignore_errors=True\n",
    "            ).select(base_columns)\n",
    "        elif Extension == \"excel\":\n",
    "            df_chunk = ExcelReadChunk(\n",
    "                origin_filepath,\n",
    "                skip_rows=pos,\n",
    "                n_rows=rows_per_partition,\n",
    "                schema_dict=schema_dict,\n",
    "                base_columns=base_columns\n",
    "            )\n",
    "\n",
    "        print(\"df_chunk\")\n",
    "        print(df_chunk)\n",
    " \n",
    "        if df_chunk.is_empty():\n",
    "            break\n",
    " \n",
    "        write_deltalake(\n",
    "            delta_dir,\n",
    "            df_chunk,\n",
    "            mode=\"overwrite\" if first_chunk else \"append\",\n",
    "            schema_mode=\"merge\"\n",
    "        )\n",
    "        first_chunk = False\n",
    "        print(f\"â†’ Written partition #{chunk_idx} with {len(df_chunk):,} rows\")\n",
    "        pos += len(df_chunk)\n",
    "        chunk_idx += 1\n",
    " \n",
    "    print(\"\\n==========================================\")\n",
    "    print(\" FULLY STREAMING CSV â†’ DELTA COMPLETED ðŸŽ‰\")\n",
    "    print(\" Delta path:\", delta_dir)\n",
    "    print(\"==========================================\\n\")\n",
    "    return delta_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    pl.Config.set_fmt_str_lengths(2000)   # widen column display\n",
    "    pl.Config.set_tbl_width_chars(2000)   # expand table width\n",
    "    pl.Config.set_tbl_rows(50000)         # show up to 50k rows\n",
    "    pl.Config.set_tbl_cols(500) \n",
    "\n",
    "    MakeParquetChunks(\n",
    "        \"abc\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\OxyzoDummyTemplateExcel.xlsx\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumeration for data types.\"\"\"\n",
    "    Unknown = 0\n",
    "    Numeric = 1\n",
    "    DateTime = 2\n",
    "    Float = 3\n",
    "    String = 4\n",
    "    Formula = 5\n",
    "    Blank = 6\n",
    "    Boolean = 7\n",
    "    Error = 8\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def map_pyarrow_type(dtype: pa.DataType) -> int:\n",
    "    if pa.types.is_integer(dtype) or pa.types.is_decimal(dtype):\n",
    "        return DataType.Numeric.value\n",
    "    if pa.types.is_floating(dtype):\n",
    "        return DataType.Float.value\n",
    "    if pa.types.is_string(dtype):\n",
    "        return DataType.String.value\n",
    "    if pa.types.is_boolean(dtype):\n",
    "        return DataType.Boolean.value\n",
    "    if pa.types.is_timestamp(dtype) or pa.types.is_date(dtype):\n",
    "        return DataType.DateTime.value\n",
    "    return DataType.Unknown.value\n",
    "\n",
    "def GetFullDetailsOfDeltaLakeFile(folder_path: str):\n",
    "    dt = DeltaTable(folder_path)\n",
    "\n",
    "    # Get a PyArrow Dataset and its (PyArrow) schema\n",
    "    dataset = dt.to_pyarrow_dataset()          # <- produces a standard PyArrow dataset\n",
    "    arrow_schema = dataset.schema              # <- PyArrow schema (pa.Schema)\n",
    "\n",
    "    row_count = 0\n",
    "    col_info = {\n",
    "        f.name: {\n",
    "            \"DataFileColumnName\": f.name,\n",
    "            \"ColumnType\": map_pyarrow_type(f.type),  # pa.DataType OK here\n",
    "            \"HasNullValue\": False,\n",
    "            \"DistinctValueCount\": set(),\n",
    "        }\n",
    "        for f in arrow_schema\n",
    "    }\n",
    "\n",
    "    scanner = ds.Scanner.from_dataset(dataset, columns=None, batch_size=50_000)\n",
    "\n",
    "    for batch in scanner.to_batches():\n",
    "        row_count += len(batch)\n",
    "        for col_name, array in zip(batch.schema.names, batch.columns):\n",
    "            if array.null_count > 0:\n",
    "                col_info[col_name][\"HasNullValue\"] = True\n",
    "            try:\n",
    "                uniques = pa.compute.unique(array.drop_null())\n",
    "                col_info[col_name][\"DistinctValueCount\"].update(uniques.to_pylist())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for col in col_info.values():\n",
    "        col[\"DistinctValueCount\"] = len(col[\"DistinctValueCount\"])\n",
    "\n",
    "    return {\n",
    "        \"StgDataFileInfo\": {\"RowCount\": row_count},\n",
    "        \"StgDataFileColumnInfo\": list(col_info.values()),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    data = GetFullDetailsOfDeltaLakeFile(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\")\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
