{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59872cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Successfully loaded Polars DataFrame sample (df_sample):\n",
      "shape: (4, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ column_1 â”† column_2    â”† column_3 â”† column_4 â”† â€¦ â”† column_16 â”† column_17 â”† column_18 â”† column_19 â”‚\n",
      "â”‚ ---      â”† ---         â”† ---      â”† ---      â”†   â”† ---       â”† ---       â”† ---       â”† ---       â”‚\n",
      "â”‚ i64      â”† str         â”† str      â”† null     â”†   â”† i64       â”† i64       â”† i64       â”† i64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 535345   â”† OXYTL01ZUBT â”† ABC123   â”† null     â”† â€¦ â”† 1         â”† 1         â”† 1         â”† 0         â”‚\n",
      "â”‚ 657557   â”† OXYTL01JEP3 â”† ABC124   â”† null     â”† â€¦ â”† 2         â”† 3         â”† 2         â”† 119134    â”‚\n",
      "â”‚ 323232   â”† OXYTL0116YS â”† ABC125   â”† null     â”† â€¦ â”† 2         â”† 2         â”† 3         â”† 0         â”‚\n",
      "â”‚ 856345   â”† OXYTL01TJ75 â”† ABC126   â”† null     â”† â€¦ â”† 3         â”† 1         â”† 4         â”† 168741    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhask\\AppData\\Local\\Temp\\ipykernel_31728\\1568739134.py:53: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  df_sample = pl.DataFrame(data, schema=[f\"column_{i+1}\" for i in range(len(data[0]))])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from deltalake import write_deltalake\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook, utils\n",
    "\n",
    "def DetectFileType(filepath: str) -> str:\n",
    "    ext = Path(filepath).suffix.lower()\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    elif ext in [\".xlsx\", \".xls\", \".xlsm\", \".xlsb\"]:\n",
    "        return \"excel\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def ExcelRead(origin_filepath: str, skip_rows: int = 0, max_sample_rows: int = 50_000) -> pl.DataFrame:\n",
    "    # --- 1. Use openpyxl to get sheet names (Introspection) ---\n",
    "    # load_workbook() with read_only=True is memory efficient.\n",
    "    try:\n",
    "        # Load the workbook structure (read-only and data_only=False)\n",
    "        # This prevents loading cell values, saving memory.\n",
    "        workbook = load_workbook(filename=origin_filepath, read_only=True, data_only=False)\n",
    "        sheet_names = workbook.sheetnames\n",
    "        \n",
    "        if not sheet_names:\n",
    "            raise ValueError(\"No sheets found in the Excel file.\")\n",
    "\n",
    "        # We will target the first sheet found\n",
    "        target_sheet_name = sheet_names[0]\n",
    "        \n",
    "        # The first row to read in Excel is 1-based, and we need to skip `skip_rows`.\n",
    "        # If skip_rows is 5, we start reading at row 6.\n",
    "        start_row = skip_rows + 1 \n",
    "        \n",
    "        # The last row to read will be: start_row + max_sample_rows - 1\n",
    "        end_row = start_row + max_sample_rows - 1\n",
    "        \n",
    "        # The Polars Excel reader defaults to reading all columns found in the sheet.\n",
    "        ws = workbook[target_sheet_name]\n",
    "        \n",
    "        # Get the raw values for the targeted row range\n",
    "        data = []\n",
    "        \n",
    "        # Only iterate through the rows we want to sample\n",
    "        for i, row in enumerate(ws.iter_rows(min_row=start_row, max_row=end_row)):\n",
    "            data.append([cell.value for cell in row])\n",
    "            \n",
    "        if data:\n",
    "            df_sample = pl.DataFrame(data, schema=[f\"column_{i+1}\" for i in range(len(data[0]))])\n",
    "        else:\n",
    "            df_sample = pl.DataFrame()\n",
    "\n",
    "        print(\"\\nâœ… Successfully loaded Polars DataFrame sample (df_sample):\")\n",
    "        print(df_sample)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the Polars-only (with openpyxl utility) Excel read: {e}\")\n",
    "        df_sample = pl.DataFrame()\n",
    "\n",
    "    finally:\n",
    "        # Ensure the workbook is closed\n",
    "        if 'workbook' in locals():\n",
    "            workbook.close()\n",
    "\n",
    "\n",
    "def MakeParquetChunks(\n",
    "    newly_created_data_file_guid: str,\n",
    "    origin_filepath: str,\n",
    "    delta_dir_network_server_path: str,\n",
    "    save_temporary_deltalake_and_copy: bool\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fully streaming CSV â†’ Delta pipeline that does NOT materialize the whole CSV.\n",
    "\n",
    "    - Reads a small sample from the file (first `max_sample_rows`) using the builtin\n",
    "      `csv` reader (no Polars full-file read) to infer schema and average row size.\n",
    "    - Streams the file with `csv.reader`, building small chunk DataFrames and\n",
    "      writing them directly to Delta (overwrite first, append subsequent).\n",
    "    - Keeps memory usage bounded by `rows_per_partition` (and the sample size).\n",
    "    \"\"\"\n",
    "\n",
    "    target_mb_per_partition: int = 512\n",
    "    skip_rows: int = 2\n",
    "    max_sample_rows: int = 50_000\n",
    "    buffer_rows: int = 50_000\n",
    "\n",
    "    # Validate inputs\n",
    "    if not origin_filepath or not os.path.isfile(origin_filepath):\n",
    "        raise FileNotFoundError(f\"Origin CSV not found: {origin_filepath}\")\n",
    "    \n",
    "    # Temporary local delta lake directory because delta lake is not being saved in network address\n",
    "    if save_temporary_deltalake_and_copy == True:\n",
    "        delta_dir = \"C:\\\\TemporaryDeltaLake\\\\\" + newly_created_data_file_guid\n",
    "    else:\n",
    "        delta_dir = delta_dir_network_server_path\n",
    "\n",
    "    # Directories preparation\n",
    "    os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Lazy scan CSV for random sample\n",
    "    # -----------------------------\n",
    "    #lf = pl.scan_csv(origin_filepath, ignore_errors=True)\n",
    "    ## Read some rows while skipping some rows to get a better sample\n",
    "    if DetectFileType(origin_filepath) == \"csv\":\n",
    "        df_sample = pl.read_csv(origin_filepath, skip_rows=skip_rows, n_rows=max_sample_rows, ignore_errors=True)\n",
    "    elif DetectFileType(origin_filepath) == \"excel\":\n",
    "        df_sample = ExcelRead(origin_filepath, skip_rows, max_sample_rows)\n",
    "\n",
    "    print(df_sample)\n",
    "\n",
    "    ## Getting the base schema\n",
    "    schema_dict = {col: dtype for col, dtype in zip(df_sample.columns, df_sample.dtypes)}\n",
    "    base_columns = list(schema_dict.keys())\n",
    " \n",
    "    # Estimate partition size\n",
    "    temp_path = \"temp_sample.parquet\"\n",
    "    df_sample.write_parquet(temp_path)\n",
    "    row_size = os.path.getsize(temp_path) / len(df_sample)\n",
    "    os.remove(temp_path)\n",
    "    rows_per_partition = max(int((target_mb_per_partition * 1024 * 1024) / row_size), buffer_rows)\n",
    "    print(f\"Target rows per partition â‰ˆ {rows_per_partition:,} \")\n",
    " \n",
    "    # -----------------------------\n",
    "    # Streaming CSV read + Delta write\n",
    "    # -----------------------------\n",
    "    first_chunk = True\n",
    "    pos = 0\n",
    "    chunk_idx = 1\n",
    " \n",
    "    ## Since the file is huge we read in chunks and write directly to Delta\n",
    "    while True:\n",
    "        print(f\"Reading rows {pos:,} â†’ {pos + rows_per_partition - 1:,} \",time.time())\n",
    "        df_chunk = pl.read_csv(\n",
    "            origin_filepath,\n",
    "            skip_rows=pos,\n",
    "            n_rows=rows_per_partition,\n",
    "            dtypes=schema_dict,\n",
    "            ignore_errors=True\n",
    "        ).select(base_columns)\n",
    " \n",
    "        if df_chunk.is_empty():\n",
    "            break\n",
    " \n",
    "        write_deltalake(\n",
    "            delta_dir,\n",
    "            df_chunk,\n",
    "            mode=\"overwrite\" if first_chunk else \"append\",\n",
    "            schema_mode=\"merge\"\n",
    "        )\n",
    "        first_chunk = False\n",
    "        print(f\"â†’ Written partition #{chunk_idx} with {len(df_chunk):,} rows\")\n",
    "        pos += len(df_chunk)\n",
    "        chunk_idx += 1\n",
    " \n",
    "    print(\"\\n==========================================\")\n",
    "    print(\" FULLY STREAMING CSV â†’ DELTA COMPLETED ðŸŽ‰\")\n",
    "    print(\" Delta path:\", delta_dir)\n",
    "    print(\"==========================================\\n\")\n",
    "    return delta_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    MakeParquetChunks(\n",
    "        \"abc\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\OxyzoDummyTemplateExcel.xlsx\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumeration for data types.\"\"\"\n",
    "    Unknown = 0\n",
    "    Numeric = 1\n",
    "    DateTime = 2\n",
    "    Float = 3\n",
    "    String = 4\n",
    "    Formula = 5\n",
    "    Blank = 6\n",
    "    Boolean = 7\n",
    "    Error = 8\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def map_pyarrow_type(dtype: pa.DataType) -> int:\n",
    "    if pa.types.is_integer(dtype) or pa.types.is_decimal(dtype):\n",
    "        return DataType.Numeric.value\n",
    "    if pa.types.is_floating(dtype):\n",
    "        return DataType.Float.value\n",
    "    if pa.types.is_string(dtype):\n",
    "        return DataType.String.value\n",
    "    if pa.types.is_boolean(dtype):\n",
    "        return DataType.Boolean.value\n",
    "    if pa.types.is_timestamp(dtype) or pa.types.is_date(dtype):\n",
    "        return DataType.DateTime.value\n",
    "    return DataType.Unknown.value\n",
    "\n",
    "def GetFullDetailsOfDeltaLakeFile(folder_path: str):\n",
    "    dt = DeltaTable(folder_path)\n",
    "\n",
    "    # Get a PyArrow Dataset and its (PyArrow) schema\n",
    "    dataset = dt.to_pyarrow_dataset()          # <- produces a standard PyArrow dataset\n",
    "    arrow_schema = dataset.schema              # <- PyArrow schema (pa.Schema)\n",
    "\n",
    "    row_count = 0\n",
    "    col_info = {\n",
    "        f.name: {\n",
    "            \"DataFileColumnName\": f.name,\n",
    "            \"ColumnType\": map_pyarrow_type(f.type),  # pa.DataType OK here\n",
    "            \"HasNullValue\": False,\n",
    "            \"DistinctValueCount\": set(),\n",
    "        }\n",
    "        for f in arrow_schema\n",
    "    }\n",
    "\n",
    "    scanner = ds.Scanner.from_dataset(dataset, columns=None, batch_size=50_000)\n",
    "\n",
    "    for batch in scanner.to_batches():\n",
    "        row_count += len(batch)\n",
    "        for col_name, array in zip(batch.schema.names, batch.columns):\n",
    "            if array.null_count > 0:\n",
    "                col_info[col_name][\"HasNullValue\"] = True\n",
    "            try:\n",
    "                uniques = pa.compute.unique(array.drop_null())\n",
    "                col_info[col_name][\"DistinctValueCount\"].update(uniques.to_pylist())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for col in col_info.values():\n",
    "        col[\"DistinctValueCount\"] = len(col[\"DistinctValueCount\"])\n",
    "\n",
    "    return {\n",
    "        \"StgDataFileInfo\": {\"RowCount\": row_count},\n",
    "        \"StgDataFileColumnInfo\": list(col_info.values()),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    data = GetFullDetailsOfDeltaLakeFile(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\")\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
