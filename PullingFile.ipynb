{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59872cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target rows per partition â‰ˆ 357,866 \n",
      "Reading rows 0 â†’ 357,865  1764614117.234116\n",
      "â†’ Written partition #1 with 5 rows\n",
      "Reading rows 5 â†’ 357,870  1764614117.2510238\n",
      "\n",
      "==========================================\n",
      " FULLY STREAMING CSV â†’ DELTA COMPLETED ðŸŽ‰\n",
      " Delta path: E:\\Dev\\Oxyzo R&D\\RndGit\\SourceDeltaLake\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhask\\AppData\\Local\\Temp\\ipykernel_5144\\2079161690.py:73: DeprecationWarning: the argument `dtypes` for `read_csv` is deprecated. It was renamed to `schema_overrides` in version 0.20.31.\n",
      "  df_chunk = pl.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from deltalake import write_deltalake\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def MakeParquetChunks(\n",
    "    newly_created_data_file_guid: str,\n",
    "    origin_filepath: str,\n",
    "    delta_dir_network_server_path: str,\n",
    "    save_temporary_deltalake_and_copy: bool\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fully streaming CSV â†’ Delta pipeline that does NOT materialize the whole CSV.\n",
    "\n",
    "    - Reads a small sample from the file (first `max_sample_rows`) using the builtin\n",
    "      `csv` reader (no Polars full-file read) to infer schema and average row size.\n",
    "    - Streams the file with `csv.reader`, building small chunk DataFrames and\n",
    "      writing them directly to Delta (overwrite first, append subsequent).\n",
    "    - Keeps memory usage bounded by `rows_per_partition` (and the sample size).\n",
    "    \"\"\"\n",
    "\n",
    "    target_mb_per_partition: int = 512\n",
    "    skip_rows: int = 0\n",
    "    max_sample_rows: int = 10\n",
    "    buffer_rows: int = 50_000\n",
    "\n",
    "    # Validate inputs\n",
    "    if not origin_filepath or not os.path.isfile(origin_filepath):\n",
    "        raise FileNotFoundError(f\"Origin CSV not found: {origin_filepath}\")\n",
    "    \n",
    "    # Temporary local delta lake directory because delta lake is not being saved in network address\n",
    "    if save_temporary_deltalake_and_copy == True:\n",
    "        delta_dir = \"C:\\\\TemporaryDeltaLake\\\\\" + newly_created_data_file_guid\n",
    "    else:\n",
    "        delta_dir = delta_dir_network_server_path\n",
    "\n",
    "    # Directories preparation\n",
    "    os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Lazy scan CSV for random sample\n",
    "    # -----------------------------\n",
    "    #lf = pl.scan_csv(origin_filepath, ignore_errors=True)\n",
    "    ## Read some rows while skipping some rows to get a better sample\n",
    "    df_sample = pl.read_csv(origin_filepath, skip_rows=skip_rows, n_rows=max_sample_rows, ignore_errors=True)\n",
    " \n",
    "    ## Getting the base schema\n",
    "    schema_dict = {col: dtype for col, dtype in zip(df_sample.columns, df_sample.dtypes)}\n",
    "    base_columns = list(schema_dict.keys())\n",
    " \n",
    "    # Estimate partition size\n",
    "    temp_path = \"temp_sample.parquet\"\n",
    "    df_sample.write_parquet(temp_path)\n",
    "    row_size = os.path.getsize(temp_path) / len(df_sample)\n",
    "    os.remove(temp_path)\n",
    "    rows_per_partition = max(int((target_mb_per_partition * 1024 * 1024) / row_size), buffer_rows)\n",
    "    print(f\"Target rows per partition â‰ˆ {rows_per_partition:,} \")\n",
    " \n",
    "    # -----------------------------\n",
    "    # Streaming CSV read + Delta write\n",
    "    # -----------------------------\n",
    "    first_chunk = True\n",
    "    pos = 0\n",
    "    chunk_idx = 1\n",
    " \n",
    "    ## Since the file is huge we read in chunks and write directly to Delta\n",
    "    while True:\n",
    "        print(f\"Reading rows {pos:,} â†’ {pos + rows_per_partition - 1:,} \",time.time())\n",
    "        df_chunk = pl.read_csv(\n",
    "            origin_filepath,\n",
    "            skip_rows=pos,\n",
    "            n_rows=rows_per_partition,\n",
    "            dtypes=schema_dict,\n",
    "            ignore_errors=True\n",
    "        ).select(base_columns)\n",
    " \n",
    "        if df_chunk.is_empty():\n",
    "            break\n",
    " \n",
    "        write_deltalake(\n",
    "            delta_dir,\n",
    "            df_chunk,\n",
    "            mode=\"overwrite\" if first_chunk else \"append\",\n",
    "            schema_mode=\"merge\"\n",
    "        )\n",
    "        first_chunk = False\n",
    "        print(f\"â†’ Written partition #{chunk_idx} with {len(df_chunk):,} rows\")\n",
    "        pos += len(df_chunk)\n",
    "        chunk_idx += 1\n",
    " \n",
    "    print(\"\\n==========================================\")\n",
    "    print(\" FULLY STREAMING CSV â†’ DELTA COMPLETED ðŸŽ‰\")\n",
    "    print(\" Delta path:\", delta_dir)\n",
    "    print(\"==========================================\\n\")\n",
    "    return delta_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    MakeParquetChunks(\n",
    "        \"abc\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\OxyzoDummyTemplate.csv\",\n",
    "        \"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\",\n",
    "        False\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4843adc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StgDataFileInfo': {'RowCount': 5}, 'StgDataFileColumnInfo': [{'DataFileColumnName': 'Loan ID', 'ColumnType': 1, 'HasNullValue': True, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Loan No.', 'ColumnType': 4, 'HasNullValue': True, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Org ID', 'ColumnType': 4, 'HasNullValue': True, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Org Name', 'ColumnType': 4, 'HasNullValue': True, 'DistinctValueCount': 0}, {'DataFileColumnName': 'Sanction Date', 'ColumnType': 4, 'HasNullValue': True, 'DistinctValueCount': 3}, {'DataFileColumnName': 'Current max DPD', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 2}, {'DataFileColumnName': 'Lifetime max DPD', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Lifetime max DPD on', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Total Overdue', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 2}, {'DataFileColumnName': 'Principal Overdue', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 2}, {'DataFileColumnName': 'EMI Overdue', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 2}, {'DataFileColumnName': 'POS', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Current max DPD_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 3}, {'DataFileColumnName': 'Lifetime max DPD_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 3}, {'DataFileColumnName': 'Lifetime max DPD on_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Total Overdue_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'Principal Overdue_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}, {'DataFileColumnName': 'EMI Overdue_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 5}, {'DataFileColumnName': 'POS_duplicated_0', 'ColumnType': 4, 'HasNullValue': False, 'DistinctValueCount': 4}]}\n"
     ]
    }
   ],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumeration for data types.\"\"\"\n",
    "    Unknown = 0\n",
    "    Numeric = 1\n",
    "    DateTime = 2\n",
    "    Float = 3\n",
    "    String = 4\n",
    "    Formula = 5\n",
    "    Blank = 6\n",
    "    Boolean = 7\n",
    "    Error = 8\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def map_pyarrow_type(dtype: pa.DataType) -> int:\n",
    "    if pa.types.is_integer(dtype) or pa.types.is_decimal(dtype):\n",
    "        return DataType.Numeric.value\n",
    "    if pa.types.is_floating(dtype):\n",
    "        return DataType.Float.value\n",
    "    if pa.types.is_string(dtype):\n",
    "        return DataType.String.value\n",
    "    if pa.types.is_boolean(dtype):\n",
    "        return DataType.Boolean.value\n",
    "    if pa.types.is_timestamp(dtype) or pa.types.is_date(dtype):\n",
    "        return DataType.DateTime.value\n",
    "    return DataType.Unknown.value\n",
    "\n",
    "def GetFullDetailsOfDeltaLakeFile(folder_path: str):\n",
    "    dt = DeltaTable(folder_path)\n",
    "\n",
    "    # Get a PyArrow Dataset and its (PyArrow) schema\n",
    "    dataset = dt.to_pyarrow_dataset()          # <- produces a standard PyArrow dataset\n",
    "    arrow_schema = dataset.schema              # <- PyArrow schema (pa.Schema)\n",
    "\n",
    "    row_count = 0\n",
    "    col_info = {\n",
    "        f.name: {\n",
    "            \"DataFileColumnName\": f.name,\n",
    "            \"ColumnType\": map_pyarrow_type(f.type),  # pa.DataType OK here\n",
    "            \"HasNullValue\": False,\n",
    "            \"DistinctValueCount\": set(),\n",
    "        }\n",
    "        for f in arrow_schema\n",
    "    }\n",
    "\n",
    "    scanner = ds.Scanner.from_dataset(dataset, columns=None, batch_size=50_000)\n",
    "\n",
    "    for batch in scanner.to_batches():\n",
    "        row_count += len(batch)\n",
    "        for col_name, array in zip(batch.schema.names, batch.columns):\n",
    "            if array.null_count > 0:\n",
    "                col_info[col_name][\"HasNullValue\"] = True\n",
    "            try:\n",
    "                uniques = pa.compute.unique(array.drop_null())\n",
    "                col_info[col_name][\"DistinctValueCount\"].update(uniques.to_pylist())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for col in col_info.values():\n",
    "        col[\"DistinctValueCount\"] = len(col[\"DistinctValueCount\"])\n",
    "\n",
    "    return {\n",
    "        \"StgDataFileInfo\": {\"RowCount\": row_count},\n",
    "        \"StgDataFileColumnInfo\": list(col_info.values()),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    data = GetFullDetailsOfDeltaLakeFile(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\")\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
