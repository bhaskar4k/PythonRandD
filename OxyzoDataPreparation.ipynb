{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4477a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "from typing import List\n",
    "\n",
    "def SaveDataFrameAsDeltaLake(\n",
    "    df: pl.DataFrame,\n",
    "    delta_dir: str,\n",
    "    target_mb_per_partition: int = 512,\n",
    "    sample_rows: int = 10,\n",
    "    buffer_rows: int = 50_000\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Saves a Polars DataFrame to a Delta Lake directory using chunked streaming.\n",
    "\n",
    "    - Estimates row size by writing first `sample_rows` to Parquet.\n",
    "    - Computes rows_per_partition based on target partition size.\n",
    "    - Writes chunk-by-chunk (overwrite â†’ append).\n",
    "    - Never loads/creates new big frames except the current chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    if df.is_empty():\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "\n",
    "    os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1. Estimate row size\n",
    "    # ----------------------------------------\n",
    "    n = min(sample_rows, len(df))\n",
    "    df_sample = df.head(n)\n",
    "\n",
    "    tmp_path = \"tmp_row_sample.parquet\"\n",
    "    df_sample.write_parquet(tmp_path)\n",
    "    row_size = os.path.getsize(tmp_path) / n\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "    # Compute rows per partition\n",
    "    rows_per_partition = max(\n",
    "        int((target_mb_per_partition * 1024 * 1024) / row_size),\n",
    "        buffer_rows\n",
    "    )\n",
    "\n",
    "    print(f\"Target rows per partition â‰ˆ {rows_per_partition:,}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. Chunked write to Delta Lake\n",
    "    # ----------------------------------------\n",
    "    first_write = True\n",
    "    total_rows = len(df)\n",
    "    chunk_idx = 1\n",
    "\n",
    "    for start in range(0, total_rows, rows_per_partition):\n",
    "\n",
    "        end = min(start + rows_per_partition, total_rows)\n",
    "        print(f\"Writing rows {start:,} â†’ {end-1:,}\")\n",
    "\n",
    "        df_chunk = df.slice(start, end - start)\n",
    "\n",
    "        write_deltalake(\n",
    "            delta_dir,\n",
    "            df_chunk,\n",
    "            mode=\"overwrite\" if first_write else \"append\",\n",
    "            schema_mode=\"merge\"\n",
    "        )\n",
    "        first_write = False\n",
    "\n",
    "        print(f\"â†’ Written partition #{chunk_idx} with {len(df_chunk):,} rows\")\n",
    "        chunk_idx += 1\n",
    "\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\" DATAFRAME â†’ DELTA COMPLETED ğŸ‰\")\n",
    "    print(\" Delta path:\", delta_dir)\n",
    "    print(\"==========================================\\n\")\n",
    "\n",
    "    return delta_dir\n",
    "\n",
    "from enum import Enum\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumeration for data types.\"\"\"\n",
    "    Unknown = 0\n",
    "    Numeric = 1\n",
    "    DateTime = 2\n",
    "    Float = 3\n",
    "    String = 4\n",
    "    Formula = 5\n",
    "    Blank = 6\n",
    "    Boolean = 7\n",
    "    Error = 8\n",
    "\n",
    "\n",
    "def TransformProvidedSourceDataOfOxyzo(delta_folder_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the wide Oxyzo Delta Lake data into a long format.  \n",
    "    It dynamically finds the EOM header row, handles N blocks, and uses Polars 'unpivot'. \n",
    "    Now with prefix matching for metrics. \n",
    "    \"\"\"\n",
    "    # 1. Load Delta Lake\n",
    "    dt = DeltaTable(delta_folder_path)\n",
    "    df = pl.from_arrow(dt. to_pyarrow_table())\n",
    "\n",
    "    print(\"Loaded DF\")\n",
    "    print(df)\n",
    "\n",
    "    # 2. Define Constants\n",
    "    FIXED_COL_COUNT = 5\n",
    "    METRIC_BLOCK_SIZE = 7\n",
    "    \n",
    "    fixed_cols = [\"Loan ID\", \"Loan No.\", \"Org ID\", \"Org Name\", \"Sanction Date\"]\n",
    "    metrics = [\n",
    "        \"Current max DPD\", \"Lifetime max DPD\", \"Lifetime max DPD on\", \n",
    "        \"Total Overdue\", \"Principal Overdue\", \"EMI Overdue\", \"POS\"\n",
    "    ]\n",
    "\n",
    "    column_type_map = {\n",
    "        \"Loan ID\": DataType.String,\n",
    "        \"Loan No.\": DataType.String,\n",
    "        \"Org ID\": DataType.String,\n",
    "        \"Org Name\": DataType.String,\n",
    "        \"Sanction Date\": DataType.DateTime,\n",
    "        \"EOM\": DataType.DateTime,\n",
    "        \"Current max DPD\": DataType.Float,\n",
    "        \"Lifetime max DPD\": DataType.Float,\n",
    "        \"Lifetime max DPD on\": DataType.DateTime,\n",
    "        \"Total Overdue\": DataType.Float,\n",
    "        \"Principal Overdue\": DataType.Float,\n",
    "        \"EMI Overdue\": DataType.Float,\n",
    "        \"POS\": DataType.Float\n",
    "    }\n",
    "\n",
    "    SortedDataFrameColumnName = \"Org ID\"\n",
    "\n",
    "    # 3. Dynamic Date Row Detection (The Core Fix)\n",
    "    date_row_index = -1\n",
    "    eom_row = None\n",
    "    \n",
    "    for row_idx in range(min(5, df.height)):\n",
    "        current_row = df.row(row_idx)\n",
    "        \n",
    "        for check_idx in range(5, 8):\n",
    "            val = current_row[check_idx]\n",
    "            if isinstance(val, str) and val.strip() and '-' in val:\n",
    "                eom_row = current_row\n",
    "                date_row_index = row_idx\n",
    "                break\n",
    "        \n",
    "        if eom_row is not None:\n",
    "            break\n",
    "    \n",
    "    if eom_row is None:\n",
    "        raise ValueError(\"âŒ Critical Error: Could not locate the EOM date header row within the first 5 rows of the Delta Lake data.\")\n",
    "\n",
    "    # 4.  Calculate N (Number of EOM blocks)\n",
    "    total_cols = df.width\n",
    "    \n",
    "    if (total_cols - FIXED_COL_COUNT) < 0 or (total_cols - FIXED_COL_COUNT) % METRIC_BLOCK_SIZE != 0:\n",
    "        raise ValueError(f\"âŒ Invalid column count ({total_cols}). Expected 5 fixed columns plus N * 7 metric columns.\")\n",
    "    \n",
    "    n_blocks = (total_cols - FIXED_COL_COUNT) // METRIC_BLOCK_SIZE\n",
    "    \n",
    "    # 5. Extract EOM Dates resiliently from the found row\n",
    "    eom_dates: List[str] = []\n",
    "    \n",
    "    for i in range(n_blocks):\n",
    "        start_index = FIXED_COL_COUNT + (i * METRIC_BLOCK_SIZE)\n",
    "        end_index = start_index + METRIC_BLOCK_SIZE\n",
    "        \n",
    "        block_slice = eom_row[start_index : end_index]\n",
    "        date_val = None\n",
    "        \n",
    "        for val in block_slice:\n",
    "            if isinstance(val, str) and val.strip() and '-' in val:\n",
    "                date_val = val.strip()\n",
    "                break\n",
    "        \n",
    "        if date_val is None:\n",
    "             raise ValueError(f\"âŒ Date found the date row (index {date_row_index}), but still failed to find date in block {i+1}. Block slice: {block_slice}\")\n",
    "            \n",
    "        eom_dates.append(date_val)\n",
    "\n",
    "    # 6. Generate New Column Names\n",
    "    new_columns = fixed_cols. copy()\n",
    "    for date in eom_dates:\n",
    "        for metric in metrics:\n",
    "            new_columns.append(f\"{date}||{metric}\")\n",
    "\n",
    "    # 7. Apply Names and Slice Data\n",
    "    df_data = df.slice(date_row_index + 1)\n",
    "    df_data = df_data.select(df_data.columns[:len(new_columns)])\n",
    "    df_data.columns = new_columns\n",
    "\n",
    "    # 8.  Unpivot (formerly melt) - Transform Wide to Long\n",
    "    df_long_vars = [c for c in new_columns if \"||\" in c]\n",
    "\n",
    "    df_unpivoted = df_data.unpivot(\n",
    "        index=fixed_cols,\n",
    "        on=df_long_vars,\n",
    "        variable_name=\"EOM_Metric\",\n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    # 9. Split EOM and Metric and Pivot Back (Standard procedure)\n",
    "    df_long = df_unpivoted.with_columns(\n",
    "        pl.col(\"EOM_Metric\").str.split_exact(\"||\", 1)\n",
    "        .struct.rename_fields([\"EOM\", \"Metric\"])\n",
    "        .alias(\"temp\")\n",
    "    ).unnest(\"temp\").drop(\"EOM_Metric\")\n",
    "\n",
    "    df_final = df_long.pivot(\n",
    "        on=\"Metric\",\n",
    "        index=fixed_cols + [\"EOM\"],\n",
    "        values=\"Value\",\n",
    "        aggregate_function=\"first\",\n",
    "        sort_columns=True\n",
    "    )\n",
    "\n",
    "    # 10. Type Casting with Prefix Matching\n",
    "    cols_to_cast = []\n",
    "    final_order = fixed_cols + [\"EOM\"] + metrics\n",
    "    \n",
    "    # Helper function for prefix matching\n",
    "    def get_matching_metric(col_name: str, metrics_list: List[str]) -> str:\n",
    "        \"\"\"Find the best prefix match for a column name from the metrics list.\"\"\"\n",
    "        for metric in metrics_list:\n",
    "            if col_name.startswith(metric) or metric in col_name:\n",
    "                return metric\n",
    "        return col_name  # Return original if no match found\n",
    "    \n",
    "\n",
    "    # Casting to datatype\n",
    "    def parse_date_column(col_name):\n",
    "        \"\"\"\n",
    "        Parse a date column with multiple format attempts and fallbacks.\n",
    "        \"\"\"\n",
    "        col = pl.col(col_name)\n",
    "        \n",
    "        # Try multiple datetime formats in sequence\n",
    "        parsed = (\n",
    "            col.str.strptime(pl. Datetime, \"%Y-%m-%d %H:%M:%S %Z\", strict=False)\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%d-%m-%Y %H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl. Datetime, \"%m-%d-%Y %H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl. Datetime, \"%Y-%m-%d\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%d-%m-%Y\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%m-%d-%Y\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%d/%m/%Y %H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%m/%d/%Y %H:%M:%S\", strict=False))\n",
    "            .fill_null(col. str.strptime(pl. Datetime, \"%d/%m/%Y\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%m/%d/%Y\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%Y/%m/%d\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%d. %m.%Y %H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%d.%m.%Y\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%Y.%m.%d\", strict=False))\n",
    "            # ISO 8601 formats\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%SZ\", strict=False))\n",
    "            .fill_null(col.str.strptime(pl. Datetime, \"%Y-%m-%dT%H:%M:%S%. fZ\", strict=False))\n",
    "            # Unix timestamp (seconds)\n",
    "            .fill_null(col.cast(pl.Float64, strict=False). cast(pl.Datetime(\"us\"), strict=False))\n",
    "        )\n",
    "        \n",
    "        # Extract just the date part\n",
    "        return parsed. dt.date(). alias(col_name)\n",
    "\n",
    "    for actual_col in df_final.columns:\n",
    "        if actual_col in column_type_map:\n",
    "            dtype = column_type_map[actual_col]\n",
    "\n",
    "            if dtype == DataType.DateTime:\n",
    "                cols_to_cast.append(parse_date_column(actual_col))\n",
    "            elif dtype in (DataType.Float, DataType.Numeric):\n",
    "                cols_to_cast.append(\n",
    "                    pl.col(actual_col).cast(pl.Float64, strict=False).alias(actual_col)\n",
    "                )\n",
    "            elif dtype == DataType.String:\n",
    "                cols_to_cast.append(\n",
    "                    pl.col(actual_col).cast(pl.Utf8, strict=False).alias(actual_col)\n",
    "                )\n",
    "            elif dtype == DataType.Boolean:\n",
    "                cols_to_cast.append(\n",
    "                    pl.col(actual_col).cast(pl.Boolean, strict=False).alias(actual_col)\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: cast to string\n",
    "                cols_to_cast.append(\n",
    "                    pl.col(actual_col).cast(pl.Utf8, strict=False).alias(actual_col)\n",
    "                )\n",
    "\n",
    "    df_final = df_final.with_columns(cols_to_cast)\n",
    "\n",
    "    df_final = df_final.select(final_order).sort(SortedDataFrameColumnName)\n",
    "\n",
    "    return df_final.select(final_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1ca4228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DF\n",
      "shape: (5, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Loan ID â”† Loan No.    â”† Org ID â”† Org Name â”† Sanction Date       â”† Current max DPD     â”† Lifetime max DPD    â”† Lifetime max DPD on â”† Total Overdue       â”† Principal Overdue   â”† EMI Overdue         â”† POS                 â”† Current max DPD_1   â”† Lifetime max DPD_1  â”† Lifetime max DPD on_1 â”† Total Overdue_1     â”† Principal Overdue_1 â”† EMI Overdue_1       â”† POS_1               â”‚\n",
      "â”‚ ---     â”† ---         â”† ---    â”† ---      â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”† ---                   â”† ---                 â”† ---                 â”† ---                 â”† ---                 â”‚\n",
      "â”‚ str     â”† str         â”† str    â”† str      â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                 â”† str                   â”† str                 â”† str                 â”† str                 â”† str                 â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ null    â”† null        â”† null   â”†          â”† null                â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-09-30 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00   â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”† 2025-08-31 00:00:00 â”‚\n",
      "â”‚ 535345  â”† OXYTL01ZUBT â”† ABC123 â”†          â”† 2021-12-27 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 1                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 1                   â”† 1                   â”† 1                   â”† 0                   â”‚\n",
      "â”‚ 657557  â”† OXYTL01JEP3 â”† ABC124 â”†          â”† 2021-12-10 00:00:00 â”† 0                   â”† 36                  â”† 2022-10-09 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 119134              â”† 2                   â”† 27                  â”† 2022-10-09 00:00:00   â”† 2                   â”† 3                   â”† 2                   â”† 119134              â”‚\n",
      "â”‚ 323232  â”† OXYTL0116YS â”† ABC125 â”†          â”† 2021-12-11 00:00:00 â”† 0                   â”† 11                  â”† 2022-03-14 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 0                   â”† 2                   â”† 12                  â”† 2022-03-14 00:00:00   â”† 2                   â”† 2                   â”† 3                   â”† 0                   â”‚\n",
      "â”‚ 856345  â”† OXYTL01TJ75 â”† ABC126 â”†          â”† 2021-12-11 00:00:00 â”† 0                   â”† 26                  â”† 2024-01-29 00:00:00 â”† 0                   â”† 0                   â”† 0                   â”† 168741              â”† 2                   â”† 27                  â”† 2024-01-29 00:00:00   â”† 3                   â”† 1                   â”† 4                   â”† 168741              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Transformed DataFrame:\n",
      "shape: (8, 13)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Loan ID â”† Loan No.    â”† Org ID â”† Org Name â”† Sanction Date â”† EOM        â”† Current max DPD â”† Lifetime max DPD â”† Lifetime max DPD on â”† Total Overdue â”† Principal Overdue â”† EMI Overdue â”† POS      â”‚\n",
      "â”‚ ---     â”† ---         â”† ---    â”† ---      â”† ---           â”† ---        â”† ---             â”† ---              â”† ---                 â”† ---           â”† ---               â”† ---         â”† ---      â”‚\n",
      "â”‚ str     â”† str         â”† str    â”† str      â”† date          â”† date       â”† f64             â”† f64              â”† date                â”† f64           â”† f64               â”† f64         â”† f64      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 535345  â”† OXYTL01ZUBT â”† ABC123 â”†          â”† 2021-12-27    â”† 2025-09-30 â”† 0.0             â”† 11.0             â”† 2022-03-14          â”† 0.0           â”† 0.0               â”† 0.0         â”† 0.0      â”‚\n",
      "â”‚ 535345  â”† OXYTL01ZUBT â”† ABC123 â”†          â”† 2021-12-27    â”† 2025-08-31 â”† 1.0             â”† 12.0             â”† 2022-03-14          â”† 1.0           â”† 1.0               â”† 1.0         â”† 0.0      â”‚\n",
      "â”‚ 657557  â”† OXYTL01JEP3 â”† ABC124 â”†          â”† 2021-12-10    â”† 2025-09-30 â”† 0.0             â”† 36.0             â”† 2022-10-09          â”† 0.0           â”† 0.0               â”† 0.0         â”† 119134.0 â”‚\n",
      "â”‚ 657557  â”† OXYTL01JEP3 â”† ABC124 â”†          â”† 2021-12-10    â”† 2025-08-31 â”† 2.0             â”† 27.0             â”† 2022-10-09          â”† 2.0           â”† 3.0               â”† 2.0         â”† 119134.0 â”‚\n",
      "â”‚ 323232  â”† OXYTL0116YS â”† ABC125 â”†          â”† 2021-12-11    â”† 2025-09-30 â”† 0.0             â”† 11.0             â”† 2022-03-14          â”† 0.0           â”† 0.0               â”† 0.0         â”† 0.0      â”‚\n",
      "â”‚ 323232  â”† OXYTL0116YS â”† ABC125 â”†          â”† 2021-12-11    â”† 2025-08-31 â”† 2.0             â”† 12.0             â”† 2022-03-14          â”† 2.0           â”† 2.0               â”† 3.0         â”† 0.0      â”‚\n",
      "â”‚ 856345  â”† OXYTL01TJ75 â”† ABC126 â”†          â”† 2021-12-11    â”† 2025-09-30 â”† 0.0             â”† 26.0             â”† 2024-01-29          â”† 0.0           â”† 0.0               â”† 0.0         â”† 168741.0 â”‚\n",
      "â”‚ 856345  â”† OXYTL01TJ75 â”† ABC126 â”†          â”† 2021-12-11    â”† 2025-08-31 â”† 2.0             â”† 27.0             â”† 2024-01-29          â”† 3.0           â”† 1.0               â”† 4.0         â”† 168741.0 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Target rows per partition â‰ˆ 922,260\n",
      "Writing rows 0 â†’ 7\n",
      "â†’ Written partition #1 with 8 rows\n",
      "\n",
      "==========================================\n",
      " DATAFRAME â†’ DELTA COMPLETED ğŸ‰\n",
      " Delta path: E:\\Dev\\Oxyzo R&D\\RndGit\\Output\\TransformedDeltaLake\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pl.Config.set_fmt_str_lengths(2000)   # widen column display\n",
    "pl.Config.set_tbl_width_chars(2000)   # expand table width\n",
    "pl.Config.set_tbl_rows(50000)         # show up to 50k rows\n",
    "pl.Config.set_tbl_cols(500) \n",
    "\n",
    "def main():\n",
    "    result = TransformProvidedSourceDataOfOxyzo(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\SourceDeltaLake\")\n",
    "    print(\"Transformed DataFrame:\"  )\n",
    "    print(result)\n",
    "\n",
    "    os.makedirs(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\Output\", exist_ok=True)\n",
    "    result.write_csv(\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\Output\\\\TransformedDeltaLake_CSV_Output.csv\")\n",
    "    \n",
    "\n",
    "    SaveDataFrameAsDeltaLake(\n",
    "        result,\n",
    "        delta_dir=\"E:\\\\Dev\\\\Oxyzo R&D\\\\RndGit\\\\Output\\\\TransformedDeltaLake\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
